{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_calls = pd.read_csv(\"../datasets/CatakPreprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trojan'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malware_calls.iloc[0]['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ldrloaddll ldrgetprocedureaddress regopenkeyex...</td>\n",
       "      <td>Trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>getsystemtimeasfiletime ntallocatevirtualmemor...</td>\n",
       "      <td>Trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ldrgetdllhandle ldrgetprocedureaddress getsyst...</td>\n",
       "      <td>Backdoor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ldrloaddll ldrgetprocedureaddress regopenkeyex...</td>\n",
       "      <td>Backdoor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldrloaddll ldrgetprocedureaddress wsastartup n...</td>\n",
       "      <td>Trojan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 api     class\n",
       "0  ldrloaddll ldrgetprocedureaddress regopenkeyex...    Trojan\n",
       "1  getsystemtimeasfiletime ntallocatevirtualmemor...    Trojan\n",
       "2  ldrgetdllhandle ldrgetprocedureaddress getsyst...  Backdoor\n",
       "3  ldrloaddll ldrgetprocedureaddress regopenkeyex...  Backdoor\n",
       "4  ldrloaddll ldrgetprocedureaddress wsastartup n...    Trojan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malware_calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trojan        1001\n",
       "Backdoor      1001\n",
       "Downloader    1001\n",
       "Worms         1001\n",
       "Virus         1001\n",
       "Dropper        891\n",
       "Spyware        832\n",
       "Adware         379\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malware_calls['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9467, 0.8592, 0.8592, 0.8746, 0.8829, 0.8592, 0.8592, 0.8592],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(malware_calls[\"class\"].value_counts())\n",
    "class_weights = (1 - (malware_calls['class'].value_counts().sort_index() / len(malware_calls))).values\n",
    "class_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT2IDX = {\n",
    "    'Virus': 0,\n",
    "    'Trojan': 1,\n",
    "    'Worms': 2,\n",
    "    'Downloader': 3,\n",
    "    'Backdoor': 4,\n",
    "    'Dropper': 5,\n",
    "    'Spyware': 6,\n",
    "    'Adware': 7,\n",
    "}\n",
    "\n",
    "IDX2CAT = {\n",
    "    0:'Virus',\n",
    "    1:'Trojan',\n",
    "    2:'Worms',\n",
    "    3:'Downloader',\n",
    "    4:'Backdoor',\n",
    "    5:'Dropper',\n",
    "    6:'Spyware',\n",
    "    7:'Adware',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LlaMa 2 7B Model Checkpoint from Hugging Face (need to be logged in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad778ad6e4f44a8afd89fed2027b0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes, label2id=CAT2IDX, id2label=IDX2CAT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(malware_calls.api, malware_calls['class'],\n",
    "test_size=0.2, random_state=75, stratify = malware_calls['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class', 'api'],\n",
       "        num_rows: 5685\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['class', 'api'],\n",
       "        num_rows: 1422\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import datasets\n",
    "\n",
    "train = Dataset.from_pandas(pd.concat([Y_train, X_train], axis=1)).remove_columns('__index_level_0__')\n",
    "validation = Dataset.from_pandas(pd.concat([Y_test, X_test], axis=1)).remove_columns('__index_level_0__')\n",
    "\n",
    "dataset = datasets.DatasetDict({\"train\": train, \"validation\": validation})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or Create Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GitRepo\\Neural-NetGuard\\LlaMa\\LlaMaCatak.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X51sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     model\u001b[39m.\u001b[39mresize_token_embeddings(\u001b[39mlen\u001b[39m(tokenizer))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X51sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# tokenized_dataset = dataset.map(tokenize_function, batched=True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X51sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X51sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Save the new tokenized dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X51sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m tokenized_dataset\u001b[39m.\u001b[39msave_to_disk(full_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X51sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaved new tokenized dataset.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    #extract text\n",
    "    text = examples['api']\n",
    "    \n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='np',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Path to the directory where the tokenized dataset will be saved or loaded\n",
    "directory_path = 'tokenized_datasets'\n",
    "file_name = 'catak_tokenized'\n",
    "full_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "# Check if the directory and file already exist\n",
    "if os.path.exists(full_path):\n",
    "    # Load the existing dataset\n",
    "    tokenized_dataset = load_from_disk(full_path)\n",
    "    print(\"Loaded the tokenized dataset.\")\n",
    "else:\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Save the new tokenized dataset\n",
    "    tokenized_dataset.save_to_disk(full_path)\n",
    "    print(\"Saved new tokenized dataset.\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    precision = load_metric(\"precision\")\n",
    "    recall = load_metric(\"recall\")\n",
    "    f1 = load_metric(\"f1\")\n",
    "    acc = load_metric(\"accuracy\")\n",
    "    mcc = load_metric(\"matthews_correlation\")\n",
    "    #auc = load_metric(\"auc\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision.compute(predictions=predictions, average = \"macro\", references=labels)[\"precision\"]\n",
    "    recall = recall.compute(predictions=predictions, average = \"macro\", references=labels)[\"recall\"]\n",
    "    f1 = f1.compute(predictions=predictions, average = \"macro\", references=labels)[\"f1\"]\n",
    "    acc = acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    mcc = mcc.compute(predictions=predictions, references=labels)[\"matthews_correlation\"]\n",
    "    auc = roc_auc_score(labels, softmax(logits, axis=1), multi_class='ovo', average='macro')\n",
    "    return {\"precision\": precision, \"recall\": recall, \"acc\": acc, \"mcc\": mcc, \"f1\": f1, \"auc\":auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Using Base Model Performance is not Ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trojan - Adware\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer.encode(malware_calls.iloc[0]['api'], return_tensors=\"pt\")\n",
    "logits = model(input).logits\n",
    "prediction = torch.argmax(logits)\n",
    "print(malware_calls.iloc[0]['class'] + \" - \" + IDX2CAT[prediction.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj'] # Only apply LORA to the query and value projections, the paper on LORA suggests that this provides the best results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (score): Linear(in_features=4096, out_features=8, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Will Only Tune %0.06 of the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,227,072 || all params: 6,611,603,456 || trainable%: 0.06393414287670189\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 2\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_name + \"-Malware-Classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GitRepo\\Neural-NetGuard\\LlaMa\\LlaMaCatak.ipynb Cell 35\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# creater trainer object\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator, \u001b[39m# this will dynamically pad examples in each batch to be equal length\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitRepo/Neural-NetGuard/LlaMa/LlaMaCatak.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
